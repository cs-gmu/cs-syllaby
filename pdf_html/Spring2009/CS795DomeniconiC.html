<html>
<body BACKGROUND="./paper.gif">
<center>
<h1>
<h2><font color=blue>
Spring 2009 CS-795: Ensemble Based Systems in Decision Making [INFS-795, IT-803]
</font></h2>
</h1>
</center>

<font color=red>
</font> 


<ul>
<li>
     <FONT SIZE=+1><b>Prerequisites</b></FONT>: INFS-755, or equivalent knowledge.
     Some programming experience is expected.  
     Students should be familiar with
     basic probability and statistics concepts, linear algebra, optimization, and multivariate
     calculus. 
  
</li>
</ul>

<ul>
<li>

     <FONT SIZE=+1><b>General Description and Preliminary List of Topics</b></FONT SIZE=+1>: <br>
This course is about combining the "opinions" of an ensemble of experts (e.g., classifiers) with the objective of
computing a new emerging "opinion" that is better than the individual ones. 
The task of improving classification accuracy by learning ensembles of classifiers is considered
as one of the most important directions in machine learning research.
Recent empirical work has shown that combining predictors can lead to significant reductions in
generalization error.
<br>
We will discuss popular ensemble
methods such as bagging, boosting, and AdaBoost. We will study the conditions under which combining multiple 
experts is beneficial; how to construct the individual components; how to select a subset of "good" experts 
from a large pool of possible components; and how to generate a consensus response from those of the individual
members. We will consider ensembles in which the components are supervised learners, unsupervised ones, or
learners with constraints. Challenges in each scenario will be discussed. 
      
</li>
</ul>


<ul>
<li>
     <FONT SIZE=+1><b>Course Format</b></FONT SIZE=+1>: <br> 
     Material from books and research papers published in major conferences and journals will be
     studied in depth. The course will include lectures by the instructor,
     presentations by students, and discussions. Students are required to study 
     the material covered in class. No textbook is required. Research papers, and handouts will
     be made available. 
     Grading will be based on homework assignments, 
     presentations, and a project. Homeworks will require 
     some programming.
     The actual format of the course will ultimately depend on the number of
     students enrolled.   
</li>
</ul>

<h3><a href="http://www.cs.gmu.edu/~carlotta/teaching/CS-795-s09/schedule-s09.html">Schedule of Classes</a>
We meet in Innovation Hall, Rm. 136, M 7:20pm - 10:00pm </h3>



<ul>
<li>
     <FONT SIZE=+1><b>List of papers</b> (UNDER CONSTRUCTION) <br>
     
     <FONT SIZE=+1> <b> Ensembles of Classifiers </b>
     <ul> <li>
     R. Polikar, <a href="http://users.rowan.edu/~polikar/RESEARCH/PUBLICATIONS/csm06.pdf">
     Ensemble Based Systems in Decision Making</a>, IEEE Circuits and Systems Magazine, 6(3), pp. 21-45, 2006.
     </ul> </li>

     <ul> <li>
     L. Breiman, <a href="http://www.springerlink.com/content/r532018156xp344g/fulltext.pdf">
     Bagging Predictors</a>, Machine Learning, 26(2), pp. 123-140, 1996.
     </ul> </li>

    <ul> <li>
     L. Breiman, <a href="http://www.springerlink.com/content/u0p06167n6173512/fulltext.pdf">
     Random Forests</a>, Machine Learning, 45, pp. 5-32, 2001.
     </ul> </li>

     <ul> <li>
     L. Breiman, <a href="http://www.springerlink.com/content/u0p06167n6173512/fulltext.pdf">
     Pasting small votes for classification in large databases and on-line</a>, Machine Learning, 36, pp. 85-103, 1999.
     </ul> </li>



</li>
</ul>



<ul>
<li>
     <FONT SIZE=+1><b>Sources</b> <br> UNDER CONSTRUCTION



</li>
</ul>
